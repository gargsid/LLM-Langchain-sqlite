from typing import Optional
from langchain.llms.base import LLM
from langchain_experimental.sql import SQLDatabaseChain
from langchain.utilities import SQLDatabase

import torch 
from typing import List, Mapping, Optional, Any
from pydantic import Field
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, AutoModelForSeq2SeqLM

device = torch.device("cuda:0" if torch.cuda.is_available() else torch.device('cpu'))
print('device:', device)

class SQLChain(LLM):
    model_folder_path: str = Field(None, alias='model_folder_path')
    model_name: str = Field(None, alias='model_name')
    model: Any = None 
    tokenizer: Any = None
    eos_token_id: Any = None

    # # all the optional arguments
    backend:        Optional[str]   = 't5'
    temp:           Optional[float] = 0.7
    top_p:          Optional[float] = 0.1
    top_k:          Optional[int]   = 40
    n_batch:        Optional[int]   = 8
    n_threads:      Optional[int]   = 4
    n_predict:      Optional[int]   = 256
    max_tokens:     Optional[int]   = 200
    repeat_last_n:  Optional[int]   = 64
    repeat_penalty: Optional[float] = 1.18
    num_return_sequences: Optional[int] = 1
    max_new_tokens: Optional[int] = 400
    do_sample: Optional[bool] = False
    num_beams: Optional[int] = 5

    def __init__(self):
        super(SQLChain, self).__init__()
        self.model_name = 'defog/sqlcoder'
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir='cache_dir')
        self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                cache_dir='cache_dir',
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                # load_in_8bit=True,
                # load_in_4bit=True,
                device_map="auto",
                use_cache=True,
            )
        self.model.to(device)
        self.eos_token_id = self.tokenizer.convert_tokens_to_ids(["```"])[0]

    @property
    def _get_model_default_parameters(self):
        return {
            "max_new_tokens": 400,
            "eos_token_id": self.eos_token_id,
            "pad_token_id": self.eos_token_id,
            "do_sample": False,
            "num_beams": 1,
            "num_return_sequences": 1,
        }

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """
        Get all the identifying parameters
        """
        return {
            'model_name' : self.model_name,
            'model_path' : self.model_folder_path,
            'model_parameters': self._get_model_default_parameters
        }

    @property
    def _llm_type(self) -> str:
        return 'sqlcoder'
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        """
        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered

        Returns:
            The string generated by the model        
        """
        inputs = self.tokenizer(prompt, return_tensors='pt').to(device)
        generated_ids = model.generate(
            **inputs,
            num_return_sequences=1,
            eos_token_id=eos_token_id,
            pad_token_id=eos_token_id,
            max_new_tokens=400,
            do_sample=False,
            num_beams=5 
        )

        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        response = response[0].split("```sql")[-1].split("```")[0].split(";")[0].strip() + ";"
        return response

db = SQLDatabase.from_uri("sqlite:///assets/Chinook.db")
llm = SQLChain()
db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)
db_chain.run("How many employees are there?")

# output = llm_chain('Who is the president of US?')
# print(output)

# db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)

# db_chain.run("How many employees are there?")
